{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8163ce-6b4c-462b-8c67-1bce6dbec6c9",
   "metadata": {},
   "source": [
    "## 모듈을 만드는 법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2fefe-6cd6-461b-8fa4-9e385009c670",
   "metadata": {},
   "source": [
    "파이토치 (라이트닝 포함) 신경망의 뼈대라고 할 수 있는 모듈을 만드는 방법은 여러가지가 있는데, 그중에서 대표적인 몇가지 정리하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a90fc3d-1357-4406-ab80-1042a2706a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3bf2f-4b39-4329-9238-7727eadacd81",
   "metadata": {},
   "source": [
    "### Lighting 의 모듈 컨테이너를 사용하는 방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019899bd-0d2b-41cc-9924-ca426f616f20",
   "metadata": {},
   "source": [
    "가장 기본적으로 사용되는 방법으로 __init__ 에서 필요한 layer들을 정의하고, 그 layer 들이 어떻게 서로 연결되는지를 forward 에서 정의하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7b1d3a-9829-425a-870c-39dda61cabec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 1]                    --\n",
       "├─Linear: 1-1                            [1, 10]                   60\n",
       "├─Linear: 1-2                            [1, 1]                    11\n",
       "==========================================================================================\n",
       "Total params: 71\n",
       "Trainable params: 71\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model( pl.LightningModule ):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(5, 10)\n",
    "        self.layer2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "summary(model, input_size = (1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c02501ec-b8a9-4537-aa29-78ad7bd3b25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (layer2): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41e21a83-0a94-4484-881d-cb42ffcd6093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=10, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941d8e9-8d1d-471e-86ba-372b0914154b",
   "metadata": {},
   "source": [
    "이렇게 만들 경우 summary 에서 보이는 layer에 직접 접근하기가 쉬울 뿐 더러 자유도가 높으며 init 과 forward 에서 순서를 알아서 지정할 수 있으므로, 병렬로 연결되거나 데이터에 따라 다른 분기를 타게 하는 등의 trick 을 사용할 때 유리하다. 또한 하나의 모듈을 다시 모듈안에 넣을 수 있는데 아래와 같은 경우를 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7390e4b6-2154-4507-a35f-453e823ccc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model2                                   [1, 1]                    --\n",
       "├─Model: 1-1                             [1, 1]                    --\n",
       "│    └─Linear: 2-1                       [1, 10]                   60\n",
       "│    └─Linear: 2-2                       [1, 1]                    11\n",
       "==========================================================================================\n",
       "Total params: 71\n",
       "Trainable params: 71\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model2( pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = Model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "model = Model2()\n",
    "summary(model, input_size=(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb36bb-6fe2-4f5d-8671-174060161905",
   "metadata": {},
   "source": [
    "위의 경우 하나의 모델이 더 큰 모델의 서브 모델로 들어갔다. 이런식으로 PyTorch 또는 PyTorch Lightning 의 모듈을 상속받아 init 과 forward 를 활용해서 사용하는 게 가장 일반적인 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6889a019-1fd9-46aa-9772-a4e6b628eb21",
   "metadata": {},
   "source": [
    "### 순차적으로 진행되는 경우 :  Sequantial container \n",
    "\n",
    "모듈의 flow 가 순차적으로 진행되는 경우 Sequantial container 를 사용할 수 있다. argument로 module의 순서를 쭉 써주기만 하면 되는데, keras의 sequential과 굉장히 흡사하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d273e523-37bd-4632-ad66-b8dc53849bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 1]                    --\n",
       "├─Sequential: 1-1                        [1, 1]                    --\n",
       "│    └─Linear: 2-1                       [1, 10]                   60\n",
       "│    └─ReLU: 2-2                         [1, 10]                   --\n",
       "│    └─Linear: 2-3                       [1, 1]                    11\n",
       "│    └─ReLU: 2-4                         [1, 1]                    --\n",
       "==========================================================================================\n",
       "Total params: 71\n",
       "Trainable params: 71\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(5, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "summary(model, input_size=(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a61a7e-6d79-487f-9d1e-4f11e3104b7b",
   "metadata": {},
   "source": [
    "이 경우 자연스럽가 앞 argument 의 결과가 다음 arugment 의 입력으로 들어간다. 또는 아예 Keras 의 add. method 와 비슷하게도 사용이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f57957b5-b3bf-4053-bc5b-b2d3cd367480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 1]                    --\n",
       "├─Sequential: 1-1                        [1, 1]                    --\n",
       "│    └─Linear: 2-1                       [1, 10]                   60\n",
       "│    └─ReLU: 2-2                         [1, 10]                   --\n",
       "│    └─Linear: 2-3                       [1, 1]                    11\n",
       "│    └─ReLU: 2-4                         [1, 1]                    --\n",
       "==========================================================================================\n",
       "Total params: 71\n",
       "Trainable params: 71\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        self.layers.add_module('linear1', nn.Linear(5, 10))\n",
    "        self.layers.add_module('ReLU1', nn.ReLU())\n",
    "        self.layers.add_module('linear2', nn.Linear(10,1))\n",
    "        self.layers.add_module('ReLU2', nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "summary(model, input_size=(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34e009-02a9-46a6-aec0-0aa8c25a9352",
   "metadata": {},
   "source": [
    "### 리스트를 활용한 Module 만들기\n",
    "\n",
    "만약 반복되는 layer를 많이 쌓아야 하는 경우 (100층이 넘는 거대 신경망의 경우) 일일이 이걸 치는 대신 list 를 사용할 수 있다. 또한 python의 가장 큰 장점중 하나인 list comprehension 을 사용할 수 있는데 다음의 예를 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d8df81f-cf26-4dce-abdf-ea7698c9e024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 10]                   --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Linear: 2-1                       [1, 10]                   110\n",
       "│    └─Linear: 2-2                       [1, 10]                   (recursive)\n",
       "│    └─Linear: 2-3                       [1, 10]                   (recursive)\n",
       "│    └─Linear: 2-4                       [1, 10]                   110\n",
       "│    └─Linear: 2-5                       [1, 10]                   (recursive)\n",
       "│    └─Linear: 2-6                       [1, 10]                   110\n",
       "│    └─Linear: 2-7                       [1, 10]                   (recursive)\n",
       "│    └─Linear: 2-8                       [1, 10]                   110\n",
       "│    └─Linear: 2-9                       [1, 10]                   (recursive)\n",
       "│    └─Linear: 2-10                      [1, 10]                   110\n",
       "==========================================================================================\n",
       "Total params: 550\n",
       "Trainable params: 550\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(5)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x)\n",
    "        return x\n",
    "    #0: x = l[0](x)+l[0](x)\n",
    "    #1: x = l[0](x)+l[1](x)\n",
    "    #2: x = l[1](x)+l[2](x)\n",
    "    #3: x = l[1](x)+l[3](x)\n",
    "    #4: x = l[2](x)+l[4](x)\n",
    "\n",
    "model = Model()\n",
    "summary(model, input_size=(1,  10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64b8ad-cdf7-4c10-af05-58fcf2fa2650",
   "metadata": {},
   "source": [
    "위와 같은 경우에는 layer 가 반복해서 사용되는데, 그 때에도 parameter 를 공유 하기 때문에 parameter 의 숫자는 매우 크진 않다. (param에서 재활용 되는 경우 recursive : 재귀적) 으로 표시된다. 물론 현실에서 이렇게 생긴 신경망으로 학습을 해야하는 경우는 보통 없긴 하지만, 굉장히 크고 복잡한 신경망을 구현하고 싶을 경우 일일이 타자를 치는 대안이 될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e03c04-2f28-4d2a-ad78-e1a76751ec1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
