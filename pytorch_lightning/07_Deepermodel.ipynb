{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b43db1-bf97-41b4-99c0-52cd12170f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchmetrics import functional as FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbb10c6-ca27-4152-9d8e-d107a0803a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(pl.LightningModule): \n",
    "     \n",
    "    def __init__(self):  ## 모델에 필요한 신경망 Layer 등 준비물은 init 에\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(28*28, 32)\n",
    "        self.linear2 = nn.Linear(28*28, 32)\n",
    "        self.linear3 = nn.Linear(32+32, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x): ## 그 신경망의 구성은 forward 에서 정의하는게 일반적임 물론, init 등에서 함수를 다 구성해두고 forward에서는 input-output 만 하게 해도 됨\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x1 = self.linear1(x)\n",
    "        x1 = self.relu(x1) \n",
    "        \n",
    "        x2 = self.linear2(x)\n",
    "        x2 = self.relu(x2)\n",
    "\n",
    "        x3 = torch.cat([x1, x2], dim=1)\n",
    "        y = self.linear3(x3) \n",
    "        \n",
    "        return(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "416f0bc3-c26e-4f15-943c-e70a305b01e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 10]                   --\n",
       "├─Flatten: 1-1                           [1, 784]                  --\n",
       "├─Linear: 1-2                            [1, 32]                   25,120\n",
       "├─ReLU: 1-3                              [1, 32]                   --\n",
       "├─Linear: 1-4                            [1, 32]                   25,120\n",
       "├─ReLU: 1-5                              [1, 32]                   --\n",
       "├─Linear: 1-6                            [1, 10]                   650\n",
       "==========================================================================================\n",
       "Total params: 50,890\n",
       "Trainable params: 50,890\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.05\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.20\n",
       "Estimated Total Size (MB): 0.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "summary(model, input_size = (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1926853-a26a-42ca-8726-093bab00f169",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "하나의 모델이 다른 모델을 하위에 가지고 있는 형태,\n",
    "\n",
    "한개의 모델에서 구성과 셋업을 다하고 바로 학습을 해도 된지만, 이렇게 할 경우 모델을 수정과 학습하는 Trainer 의 수정이 얽히게 된다.\n",
    "간단한 구조일 때에는 별로 상관 없지만, 복잡한 경우를 대비해서 (그리고 여러가지 모델을 각각 테스트하는 등 많은 노가다가 필요할 경우)\n",
    "\n",
    "모델은 여러개를 만들고\n",
    "\n",
    "학습하는 애가 model1.forward 를 쓰다가 model2.forward 를 쓰는 식으로 간단히 바꿀 수 있도록 하는게 좋기 때문에 아래처럼\n",
    "\n",
    "신경망의 구성을 하는 부분과, 학습하는 step부분을 정의한 클래스를 각각 두는게 유리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4b455b-379a-4526-a288-35d5f7793065",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ftn = nn.CrossEntropyLoss()\n",
    "\n",
    "class MyModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = Model()  ## 1개의 모델을 통으로 가지고 있는 형태 (상속이랑은 미묘하게 다름 -> 여러개 모델을 동시에 거느리는걸 생각해보자)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return(out)\n",
    "\n",
    "\n",
    "### 아래 step 부분은 우리가 직접 call 해서 쓸 수 도 있지만, 대부분(학습을 할 때 등)에는 Trainer 가 call 해서 사용한다.  따라서 거기에 맞게 \n",
    "### arguments를 맞춰줘야 하기 때문에 사용하지 않는 batch_idx 등이 arg 에 포함됨.\n",
    "\n",
    "    def predict_step(self, x, batch_idx):  ## pred 에서는 x 만 들어오기 때문에 batch 대신 x 라고 표시 \n",
    "        y_pred = self(x) # 여기까진  logit \n",
    "        y_prob = nn.Softmax(y_pred)  # 확률로 변환 \n",
    "        return(y_prob)\n",
    "\n",
    "     \n",
    "    def traning_step(self, batch, batch_idx): ## 학습시에는 (x, y) 쌍이 들어오므로 batch 라고 표현\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        acc = FM.accuracy(y_pred, y, task = 'multiclass', num_classes = 10) \n",
    "        mse = FM.mean_squared_error( torch.argmax( y_pred, dim=1 ), y)\n",
    "\n",
    "        metrics = {'loss' : loss, 'acc' : acc, 'mse' : mse }\n",
    "        self.log_dict( metrics, prog_bar = True) \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = loss_ftn(y_hat, y)    \n",
    "        acc = FM.accuracy(y_hat, y, task = 'multiclass', num_classes = 10) \n",
    "        mse = FM.mean_squared_error( torch.argmax( y_hat, dim=1 ), y)\n",
    "        metrics = {'val_loss' : loss, 'val_acc' : acc, 'val_mse' : mse }\n",
    "        self.log_dict( metrics, prog_bar = True) \n",
    "        return \n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = loss_ftn(y_hat, y)    \n",
    "        acc = FM.accuracy(y_hat, y, task = 'multiclass', num_classes = 10) \n",
    "        mse = FM.mean_squared_error( torch.argmax( y_hat, dim=1 ), y)\n",
    "        metrics = {'test_loss' : loss, 'test_acc' : acc, 'test_mse' : mse }\n",
    "        self.log_dict( metrics, prog_bar = True) \n",
    "        return \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam( self.parameters(), lr=0.001 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14829de-4900-42c2-9db3-6e38280e105a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [1, 10]                   --\n",
       "├─Model: 1-1                             [1, 10]                   --\n",
       "│    └─Flatten: 2-1                      [1, 784]                  --\n",
       "│    └─Linear: 2-2                       [1, 32]                   25,120\n",
       "│    └─ReLU: 2-3                         [1, 32]                   --\n",
       "│    └─Linear: 2-4                       [1, 32]                   25,120\n",
       "│    └─ReLU: 2-5                         [1, 32]                   --\n",
       "│    └─Linear: 2-6                       [1, 10]                   650\n",
       "==========================================================================================\n",
       "Total params: 50,890\n",
       "Trainable params: 50,890\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.05\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.20\n",
       "Estimated Total Size (MB): 0.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619a19d-1fbc-4c72-b105-da526bbcf4ce",
   "metadata": {},
   "source": [
    "모델을 만드는 법을 간단하게 복습해보았다. 이제, 다음장에서 이 모델을 학습하는 데이터 로드 모듈까지 ㄱㄱ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a2648",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
