{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ca68c85-c1c0-4fa0-a51b-944af1be2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.accelerators import accelerator\n",
    "from torchmetrics import functional as FM\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [5, 3.5]\n",
    "plt.rcParams[\"font.size\"] = \"8\"\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def dataLoader(batch_size=128):\n",
    "    train_dataset = MNIST('', transform=transforms.ToTensor(), train=True, download=True) ## 한 번 인터넷으로 가져온걸 매번 가져올 필요가 없기 때문에 가져올때 download True 로 하면 다음 부터는 다운로드 된 데이터를 사용한다.\n",
    "    test_dataset = MNIST('', transform=transforms.ToTensor(), train=False, download=True)\n",
    "    trainDataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valDataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return (trainDataLoader,valDataLoader)\n",
    "\n",
    "trainDataLoader,valDataLoader = dataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f700da8-48f4-4173-b7ea-6f8150446571",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() ## 회귀가 아닌 분류의 문제이므로 cross entropy 를 loss 로 사용한다. \n",
    "\n",
    "class mymodel( pl.LightningModule ):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 28*28, 10 ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d35116-beeb-4271-a62c-15c8dbaa2a2f",
   "metadata": {},
   "source": [
    "# 모델 리뷰 \n",
    "\n",
    "바로 전번에 돌린 모델을 보면, 이상한 점이 두 개 있다. 전에 사용했던 keras의 경우 categorical data의 cross entropy 를 계산하기 위해서는 output 되는 값을 확률 분포로 바꿔주고 (logit -> softmax 를 통한 prob), 또한 target y 값도 one-hot encoding 을 통해서 같은 dimmension 으로 바꿔줘야 확률분포의 차이 즉 크로스 엔트로피가 계산이 되었다. 근데 여기서는 그냥 y_pred (선형결합으로 이루어진 ~ logit 값, dim=10)  과  y (target 값, dim=1) 를 그냥 넣어주었다. \n",
    "\n",
    "이게 가능한 이유는 파이토치에서 이 부분을 자동으로 해주기 때문이다. 다음을 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c70c16f-0af8-4274-8ffc-eecc658003c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1672241687774658"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 0.5, 0.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee6326-fe95-4f84-a38f-171baad4d1c1",
   "metadata": {},
   "source": [
    "이걸 softmax를 취하지 않은 형태의 tensor에 대해서 적용해도 별 문제 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2365c15-7199-4236-85d9-d4fdad10a945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1672241687774658"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "softmax_layer = nn.Softmax(dim=1)\n",
    "tensor_softmaxed = softmax_layer(tensor1)  ## [[0, 0.5, 0.5, 0]] 으로 변환됨 \n",
    "\n",
    "out = loss_function(tensor_softmaxed, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e9f32-fe36-4774-9880-f5d711a77569",
   "metadata": {},
   "source": [
    "물론 정확히 Softmax를 취한 값과는 값이 살짝 다른데 이유는 cross entropy 에서 실행되는 softmax 는 log_softmax 이기 때문이다. (하지만 어차피 loss 는 경향을 보는 거지 절대값 자체가 중요한건 아니기 때문에..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34a18e36-9e99-4de0-a9f3-1e4000fb9009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cd8b4-3a5e-45c9-87f0-746b7e19faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "softmax_layer = nn.LogSoftmax(dim=1)\n",
    "tensor_softmaxed = softmax_layer(tensor1)  ## [[0, 0.5, 0.5, 0]] 으로 변환됨 \n",
    "\n",
    "out = loss_function(tensor_softmaxed, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15d5f1-70c4-459d-a97e-cac3a8275daa",
   "metadata": {},
   "source": [
    "그런데 더 놀라운 사실은 이 뿐만 아니라 one-hot encoding도 알아서 해준다. (true_y 가 pred_y 와 같은 shape 이면 그대로 확률을 비교하고, 그렇지 않다면 one-hot encoding을 진행한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6345379-5c0e-4fad-9e6d-494789d886bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [1] )  ## 확률 벡터가 아니라, 그냥 index만 넣어줘도! \n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988aa1ec-ffee-45fd-a6d4-b1caf47e65f7",
   "metadata": {},
   "source": [
    "참 편리하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c138f0fa-bb3e-4836-970c-4a15f78c06a1",
   "metadata": {},
   "source": [
    "## 모댈 개선\n",
    "\n",
    "모델을 돌려서 학습까지 했지만, 뭐 나온게 없으므로 현 수준은 그냥 에러가 나지 않고 뭔가가 돌아갔다는 데 위안을 둬야한다. 이제 실제 성능을 확인하기 위해 몇가지를 개선하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f0a2c-f623-4bc9-b6f3-318e0869b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class mymodel2( pl.LightningModule ):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 28*28, 10 ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        acc = FM.accuracy(y_pred, y, task=\"multiclass\",num_classes=10)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2f53a-1047-49c9-b2b4-122d059e8f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
