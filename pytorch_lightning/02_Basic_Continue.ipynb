{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca68c85-c1c0-4fa0-a51b-944af1be2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.accelerators import accelerator\n",
    "from torchmetrics import functional as FM\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [5, 3.5]\n",
    "plt.rcParams[\"font.size\"] = \"8\"\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def dataLoader(batch_size=128):\n",
    "    train_dataset = MNIST('', transform=transforms.ToTensor(), train=True, download=True) ## 한 번 인터넷으로 가져온걸 매번 가져올 필요가 없기 때문에 가져올때 download True 로 하면 다음 부터는 다운로드 된 데이터를 사용한다.\n",
    "    test_dataset = MNIST('', transform=transforms.ToTensor(), train=False, download=True)\n",
    "    trainDataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valDataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return (trainDataLoader,valDataLoader)\n",
    "\n",
    "trainDataLoader,valDataLoader = dataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f700da8-48f4-4173-b7ea-6f8150446571",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() ## 회귀가 아닌 분류의 문제이므로 cross entropy 를 loss 로 사용한다. \n",
    "\n",
    "class mymodel( pl.LightningModule ):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 28*28, 10 ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d35116-beeb-4271-a62c-15c8dbaa2a2f",
   "metadata": {},
   "source": [
    "# 모델 리뷰 \n",
    "\n",
    "바로 전번에 돌린 모델을 보면, 이상한 점이 두 개 있다. 전에 사용했던 keras의 경우 categorical data의 cross entropy 를 계산하기 위해서는 output 되는 값을 확률 분포로 바꿔주고 (logit -> softmax 를 통한 prob), 또한 target y 값도 one-hot encoding 을 통해서 같은 dimmension 으로 바꿔줘야 확률분포의 차이 즉 크로스 엔트로피가 계산이 되었다. 근데 여기서는 그냥 y_pred (선형결합으로 이루어진 ~ logit 값, dim=10)  과  y (target 값, dim=1) 를 그냥 넣어주었다. \n",
    "\n",
    "이게 가능한 이유는 파이토치에서 이 부분을 자동으로 해주기 때문이다. 다음을 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c70c16f-0af8-4274-8ffc-eecc658003c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1672241687774658"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 0.5, 0.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee6326-fe95-4f84-a38f-171baad4d1c1",
   "metadata": {},
   "source": [
    "이걸 softmax를 취하지 않은 형태의 tensor에 대해서 적용해도 별 문제 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e9f32-fe36-4774-9880-f5d711a77569",
   "metadata": {},
   "source": [
    "물론 정확히 Softmax를 취한 값과는 값이 살짝 다른데 이유는 cross entropy 에서 실행되는 softmax 는 log_softmax 이기 때문이다. (하지만 어차피 loss 는 경향을 보는 거지 절대값 자체가 중요한건 아니기 때문에..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a18e36-9e99-4de0-a9f3-1e4000fb9009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475cd8b4-3a5e-45c9-87f0-746b7e19faff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "softmax_layer = nn.LogSoftmax(dim=1)\n",
    "tensor_softmaxed = softmax_layer(tensor1)  ## [[0, 0.5, 0.5, 0]] 으로 변환됨 \n",
    "out = loss_function(tensor_softmaxed, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15d5f1-70c4-459d-a97e-cac3a8275daa",
   "metadata": {},
   "source": [
    "그런데 더 놀라운 사실은 이 뿐만 아니라 one-hot encoding도 알아서 해준다. (true_y 가 pred_y 와 같은 shape 이면 그대로 확률을 비교하고, 그렇지 않다면 one-hot encoding을 진행한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6345379-5c0e-4fad-9e6d-494789d886bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [1] )  ## 확률 벡터가 아니라, 그냥 index만 넣어줘도! \n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988aa1ec-ffee-45fd-a6d4-b1caf47e65f7",
   "metadata": {},
   "source": [
    "참 편리하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c138f0fa-bb3e-4836-970c-4a15f78c06a1",
   "metadata": {},
   "source": [
    "## 모델 개선\n",
    "\n",
    "모델을 돌려서 학습까지 했지만, 뭐 나온게 없으므로 현 수준은 그냥 에러가 나지 않고 뭔가가 돌아갔다는 데 위안을 둬야한다. 이제 실제 성능을 확인하기 위해 몇가지를 개선하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f7f0a2c-f623-4bc9-b6f3-318e0869b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() ## 회귀가 아닌 분류의 문제이므로 cross entropy 를 loss 로 사용한다. \n",
    "\n",
    "class mymodel2( pl.LightningModule ):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 28*28, 10 ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        acc = FM.accuracy(y_pred, y, task=\"multiclass\",num_classes=10) ## accuracy 를 계산해서\n",
    "        self.log_dict({'loss':loss, 'acc':acc})  # loss 와 함께 log 에 저장한다. \n",
    "        return loss \n",
    "         \n",
    "    def validation_step(self, batch, batch_idx):   ## training 과 같은 꼴이지만 역전파를 하지 않기 때문에 loss를 리턴하지 않는다.\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        acc = FM.accuracy(y_pred, y, task=\"multiclass\",num_classes=10)\n",
    "        self.log_dict({'val_loss':loss, 'val_acc':acc}) # 한 epoch 이 끝나면 validation 이 돌고, 이 때 역시 log에 결과를 저장한다.\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc192345-fe84-4f61-88f0-7dae8793a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = mymodel2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c2f53a-1047-49c9-b2b4-122d059e8f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 7.9 K \n",
      "--------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f07922c07f4121a6ed6092ec1ebf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python3_11_8/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/python3_11_8/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d565b3ac8d403d892d6e36b79186e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7a602934ea491693b6322777905ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35765904cc24672a0a52d7e4d08aa17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992bdf967d824a54a31bdeb8ac8ffe7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.87 s, sys: 5.26 s, total: 14.1 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 3\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"firstModel\")  ## 결과를 확인하기 위해 log 를 저장했다. \n",
    "trainer = pl.Trainer(max_epochs=epochs, logger=logger, accelerator=\"auto\") # auto 로 설장하면 gpu 가 있으면 gpu를 사용해서 실행된다.\n",
    "trainer.fit(mm, trainDataLoader, valDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e0f7964-3183-4f92-8dc9-9fea42e72914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>step</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.882812</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.843750</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.882812</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.843750</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.890625</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.882812</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.914062</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.906250</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>468</td>\n",
       "      <td>0.9021</td>\n",
       "      <td>0.379996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.906250</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.906250</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.945312</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.898438</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.914062</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.914062</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.906250</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.914062</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>937</td>\n",
       "      <td>0.9140</td>\n",
       "      <td>0.317711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.867188</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.867188</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.906250</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.953125</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.867188</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.953125</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.859375</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.906250</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.929688</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.921875</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1406</td>\n",
       "      <td>0.9181</td>\n",
       "      <td>0.295831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         acc  epoch  loss  step  val_acc  val_loss\n",
       "0   0.882812      0   1.0    49      NaN       NaN\n",
       "1   0.843750      0   1.0    99      NaN       NaN\n",
       "2   0.882812      0   1.0   149      NaN       NaN\n",
       "3   0.843750      0   1.0   199      NaN       NaN\n",
       "4   0.890625      0   1.0   249      NaN       NaN\n",
       "5   0.882812      0   1.0   299      NaN       NaN\n",
       "6   0.914062      0   1.0   349      NaN       NaN\n",
       "7   0.906250      0   1.0   399      NaN       NaN\n",
       "8   0.875000      0   1.0   449      NaN       NaN\n",
       "9        NaN      0   NaN   468   0.9021  0.379996\n",
       "10  0.906250      1   1.0   499      NaN       NaN\n",
       "11  0.937500      1   1.0   549      NaN       NaN\n",
       "12  0.906250      1   1.0   599      NaN       NaN\n",
       "13  0.945312      1   1.0   649      NaN       NaN\n",
       "14  0.898438      1   1.0   699      NaN       NaN\n",
       "15  0.914062      1   1.0   749      NaN       NaN\n",
       "16  0.914062      1   1.0   799      NaN       NaN\n",
       "17  0.906250      1   1.0   849      NaN       NaN\n",
       "18  0.914062      1   1.0   899      NaN       NaN\n",
       "19       NaN      1   NaN   937   0.9140  0.317711\n",
       "20  0.867188      2   1.0   949      NaN       NaN\n",
       "21  0.867188      2   1.0   999      NaN       NaN\n",
       "22  0.906250      2   1.0  1049      NaN       NaN\n",
       "23  0.953125      2   1.0  1099      NaN       NaN\n",
       "24  0.867188      2   1.0  1149      NaN       NaN\n",
       "25  0.953125      2   1.0  1199      NaN       NaN\n",
       "26  0.859375      2   1.0  1249      NaN       NaN\n",
       "27  0.906250      2   1.0  1299      NaN       NaN\n",
       "28  0.929688      2   1.0  1349      NaN       NaN\n",
       "29  0.921875      2   1.0  1399      NaN       NaN\n",
       "30       NaN      2   NaN  1406   0.9181  0.295831"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_num = logger.version  ## 버전에 따라서 폴더가 구분되므로, 최근에 돌린 버전을 확인 \n",
    "history = pd.read_csv(f'./logs/firstModel/version_{v_num}/metrics.csv') ## 파일 읽어오기 \n",
    "history ## 결과 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc0ac8-755d-4e50-b2d7-ece3e9153587",
   "metadata": {},
   "source": [
    "epoch에서 batch 단위로 로그가 저장됨을 알 수 있다. 기본적으로 training 이 한바퀴 돌고 validation 이 실행된다.\n",
    "epoch 단위로만 보고 싶다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147ba7d4-b1e3-4c01-ac8b-8273200785e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9021</td>\n",
       "      <td>0.379996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.914062</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9140</td>\n",
       "      <td>0.317711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.921875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9181</td>\n",
       "      <td>0.295831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc  loss  val_acc  val_loss\n",
       "epoch                                   \n",
       "0      0.875000   1.0   0.9021  0.379996\n",
       "1      0.914062   1.0   0.9140  0.317711\n",
       "2      0.921875   1.0   0.9181  0.295831"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.groupby('epoch').last().drop('step', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46513a-987e-492e-aea3-7c8d32c3ef45",
   "metadata": {},
   "source": [
    "MNIST 의 경우 너무 분류가 너무 쉬운 코드이기 때문에 별다른 trick 없이 10개의 dense layer 만으로도 90% 의 정분류율을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a9bf7-a9ec-4881-a0eb-1e2e7f6c556d",
   "metadata": {},
   "source": [
    "실제 어떻게 예측했는지를 좀 더 detail 하게 확인하기 위해서 validation 데이터를 하나 꺼내본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ef87b7-e4ba-4c9d-b318-f2683c136803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x173f36150>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAE1CAYAAACcI5eKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAATX0lEQVR4nO3df2zU933H8de5hmMG37k1cBv1mdMldlgCzbGK1G0hGIlsDj8UJkB0IgiqZo42ItopIsKVumzawKJNQXFCpP5TmjqdR4KRSJsyJW2aICuM0NGYsNDImXE4C2yTa+Jz03Dx5b77g3ErwfZ9z/46d37n+ZC+f9z5fZ/vW3zRy5/vfb+fr32O4zgCAKNKCt0AAEwmQg6AaYQcANMIOQCmEXIATCPkAJhGyAEwjZADYFppoRuQpEwmo4sXL6q8vFw+n6/Q7QAoco7jaGhoSPPmzVNJydhztaIIuYsXLyocDhe6DQBTTDweV1VV1Zg1noZcV1eXtm7dqnfeeUcVFRX60Y9+pFtvvTXn58rLyyVJS7VKpZrmZUsADEprWB36eTY7xuJpyN1///1qbGzUtm3bdPjwYX3jG9/QiRMncn7u2ilqqaap1EfIAcjh/1bcu/l6y7MLDwMDAzp9+rTuvfdeSdL69et1/vx59fT0eLULAMibZzO5eDyuefPmqbT06pA+n0/V1dW6cOGCIpHIdbWpVEqpVCr7OplMetUGAFzH01tIPj51HO0pTs3NzQoGg9mNiw4AJotnIRcOh9Xb26t0Oi3pasDF43FVV1ffUNvU1KTBwcHsFo/HvWoDAK7jWcjNnTtXixcv1lNPPSVJam9vVyQSueFUVZL8fr8CgcB1GwBMBk+vrv7gBz/Qtm3btGfPHgUCAT355JNeDg8AefM05G655RZXt4wAwCeFtasATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANMIOQCmEXIATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANMIOQCmEXIATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANMIOQCmEXIATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANMIOQCmEXIATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANM8DblIJKIFCxYoFospFovp0KFDXg4PAHkr9XrAw4cPa+HChV4PCwDjwukqANM8n8lt3rxZmUxGX/rSl9Tc3Kw5c+bcUJNKpZRKpbKvk8mk120AgCSPZ3LHjx9XZ2enTp8+rcrKSm3dunXEuubmZgWDwewWDoe9bAMAsnyO4ziTMfClS5dUW1uroaGhG3420kwuHA6rXveo1DdtMtoBYEjaGdZLOqrBwUEFAoExaz07XX3//fc1PDysiooKSVJbW5sWL148Yq3f75ff7/dq1wAwKs9Crr+/X+vXr9dHH30kx3EUjUb14x//2KvhAWBcPAu5aDSq3/zmN14NBwCe4BYSAKYRcgBMI+QAmEbIATCNkANgGiEHwDRCDoBphBwA0zx/CgnGlvjbL+esqd7ylquxfjsQclX3YSr3euDPt7lbM1zW+3tXdZnX3nBVB0w2ZnIATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANMIOQCmEXIATGPFwyfsoZ3/lrNm/cx33Q120wSb+WP17sp60n9wVffo5RXj7+VT6tWB+TlrZn4/6Gqs0l/+10TbMYOZHADTCDkAphFyAEwj5ACYRsgBMI2QA2AaIQfANEIOgGmEHADTWPHwCWv59tdy1vzjF9z97vnsOcdV3bt/7stZM/0L77ka67sLj7iq2/9nJ3PWPPeHWa7GWl3m7u9KeOkD50NXdSdTM3PW1M8YdrdTF/9mN2+639VQtb90t8tPA2ZyAEwj5ACYRsgBMI2QA2AaIQfANEIOgGmEHADTCDkApnEz8Cds5uHcN3zOPOztPgMejvXYn9a7qvvXr0Zy1gRefsvVWN+tv9lVnZdKP8i4qpt55lLOmsrj7a7GWjR9Ws6asp7cNbgeMzkApuUVcjt27FAkEpHP59PZs2ez7w8MDKihoUE1NTVauHChOjo6PG8UAMYjr5DbsGGDOjo6NH/+9X9VaNeuXaqrq1NXV5cOHjyozZs3K51Oe9ooAIxHXt/J3XnnnSO+//TTT+v8+fOSpCVLligUCqmjo0P19fUTbhAAJmLCFx4SiYQymYzmzJmTfS8SiejChQujfiaVSimVSmVfJ5PJibYBACPy5MKDz3f9o3wcZ+xHADU3NysYDGa3cDjsRRsAcIMJh1xlZaUk6fLly9n33n77bVVXV4/6maamJg0ODma3eDw+0TYAYESezOQ2btyoAwcOSJJOnTqlvr4+LV26dNR6v9+vQCBw3QYAkyGvkNu+fbuqqqrU29urlStX6uabr96kuXfvXr3yyiuqqanRtm3b1NraqtJS7jMGUHg+J9cXaJ+AZDKpYDCoet2jUh93dGPqSNz35Zw1J/75cVdj7fvdgpw1x//yJldjpS/1uaqbqtLOsF7SUQ0ODuY8E2TFAwDTCDkAphFyAEwj5ACYRsgBMI2QA2AaIQfANEIOgGmEHADTWHsFjKB0vrsn4zz+7dyrGab5PuNqrGceXZmzpvLSCVdj4f8xkwNgGiEHwDRCDoBphBwA0wg5AKYRcgBMI+QAmEbIATCNm4GBEfz2Hz7vqm6J35ez5r8//MDVWJ974w+u6pAfZnIATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANMIOQCmEXIATGPFAz5VUquXuKo7vWG/yxH9OSv+7pvfdDXSn7zyqst9Ih/M5ACYRsgBMI2QA2AaIQfANEIOgGmEHADTCDkAphFyAEwj5ACYxooHfKpcuNvd7/VZvtwrGSTpb87flbOm7D86XY3luKpCvvKaye3YsUORSEQ+n09nz57Nvl9fX69oNKpYLKZYLKb9+90uiQGAyZXXTG7Dhg166KGHtHTp0ht+1tLSojVr1njWGAB4Ia+Qu/POOyerDwCYFJ5deNi5c6cWLVqkTZs2qbu7e8zaVCqlZDJ53QYAk8GTkGttbdW5c+d05swZLVu2LOdpa3Nzs4LBYHYLh8NetAEAN/Ak5K6FlM/n0wMPPKDu7m4lEolR65uamjQ4OJjd4vG4F20AwA0mfAtJOp1WIpFQKBSSJLW3tysUCqmysnLUz/j9fvn97i7RA8BE5BVy27dv19GjR9XX16eVK1dq1qxZ6uzs1OrVq5VKpVRSUqLZs2fr2Wefnax+ASAvPsdxCn4PYjKZVDAYVL3uUalvWqHbwRRVUl6es+aOjt+5Gutbnzvlqu6v78/9aHP/z92NBffSzrBe0lENDg4qEAiMWcuyLgCmEXIATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANMIOQCm8fhzmNH1T7flrPnZ7CdcjXVP13pXdaxmKH7M5ACYRsgBMI2QA2AaIQfANEIOgGmEHADTCDkAphFyAEwj5ACYxooHFL3Be+tc1Z3Z1JKz5n/Sw67G+v3eKld1fl1yVYfCYSYHwDRCDoBphBwA0wg5AKYRcgBMI+QAmEbIATCNkANgGjcDo2BKPz/PVd23vnPIVZ3fl/u/89c6t7gaa84xHmtuBTM5AKYRcgBMI+QAmEbIATCNkANgGiEHwDRCDoBphBwA0wg5AKax4gGTwlea+7/W7T/rdTXWxlkJV3U/GZqbsyb0HXe/1zOuqjAV5DWTu3LlitatW6fa2lrFYjE1NDSop6dHkjQwMKCGhgbV1NRo4cKF6ujomIx+ASAveZ+uNjY26s0339Rrr72mNWvWqLGxUZK0a9cu1dXVqaurSwcPHtTmzZuVTqc9bxgA8pFXyM2YMUOrVq2Sz+eTJNXV1am7u1uS9PTTT2v79u2SpCVLligUCjGbA1BwE/pOrqWlRWvXrlUikVAmk9GcOXOyP4tEIrpw4cKIn0ulUkqlUtnXyWRyIm0AwKjGfXV1z5496urq0u7duyUpO7u7xnGcUT/b3NysYDCY3cLh8HjbAIAxjSvkHnnkER05ckTHjh1TWVmZKisrJUmXL1/O1rz99tuqrq4e8fNNTU0aHBzMbvF4fDxtAEBOeYfcvn371NbWphdeeEEVFRXZ9zdu3KgDBw5Ikk6dOqW+vj4tXbp0xDH8fr8CgcB1GwBMhry+k+vt7dWDDz6oaDSqFStWSLoaWCdPntTevXu1ZcsW1dTUaPr06WptbVWpi3ulAGAy5ZVCVVVVo37XFgqF9Pzzz3vSFAy4/ZacJf8yt9XTXR7YszFnTUXnCU/3ieLHsi4AphFyAEwj5ACYRsgBMI2QA2AaIQfANEIOgGmEHADTCDkAprHuCnn5zK21ruoa//2oZ/u89YfbXdVFWv/Ts33CDmZyAEwj5ACYRsgBMI2QA2AaIQfANEIOgGmEHADTCDkAphFyAExjxQPy8tu//6yrurVl3v3B8KqXPnRXOMbf+sWnFzM5AKYRcgBMI+QAmEbIATCNkANgGiEHwDRCDoBphBwA07gZGJKkK2vvcFX3y7Xfdzli2fibATzETA6AaYQcANMIOQCmEXIATCPkAJhGyAEwjZADYBohB8A0Qg6Aaax4gCTp4lc/46quutS7lQw/GZrrqm5a0t3jz3n4OUaS10zuypUrWrdunWpraxWLxdTQ0KCenh5JUn19vaLRqGKxmGKxmPbv3z8Z/QJAXvKeyTU2Nuruu++Wz+fT448/rsbGRj3//POSpJaWFq1Zs8bzJgFgvPKayc2YMUOrVq2Sz+eTJNXV1am7u3tSGgMAL0zowkNLS4vWrl2bfb1z504tWrRImzZtGjP8UqmUksnkdRsATIZxh9yePXvU1dWl3bt3S5JaW1t17tw5nTlzRsuWLRvztLW5uVnBYDC7hcPh8bYBAGMaV8g98sgjOnLkiI4dO6aysqtX264Flc/n0wMPPKDu7m4lEokRP9/U1KTBwcHsFo/Hx9k+AIwt7wsP+/btU1tbm37xi1+ooqJCkpROp5VIJBQKhSRJ7e3tCoVCqqysHHEMv98vv98//q4BwKW8Qq63t1cPPvigotGoVqxYIelqYL344otavXq1UqmUSkpKNHv2bD377LOT0jAA5COvkKuqqpLjjHzL5a9//WtPGgIAL7HiAZOiOXFrzpoTfxVxNZZz6fUJdoNPM9auAjCNkANgGiEHwDRCDoBphBwA0wg5AKYRcgBMI+QAmMbNwJAkRXedcFW3atdfeLjXPg/HAkbGTA6AaYQcANMIOQCmEXIATCPkAJhGyAEwjZADYBohB8C0orgZ+Noj1dMalkZ+ujoAZKU1LEmj/jmGP1YUITc0NCRJ6tDPC9wJgKlkaGhIwWBwzBqf4yYKJ1kmk9HFixdVXl4un88nSUomkwqHw4rH4woEAgXuMH/0X1j0X1iT3b/jOBoaGtK8efNUUjL2t25FMZMrKSlRVVXViD8LBAJT8iBfQ/+FRf+FNZn955rBXcOFBwCmEXIATCvakPP7/Xr44Yfl9/sL3cq40H9h0X9hFVP/RXHhAQAmS9HO5ADAC4QcANMIOQCmFWXIdXV16Stf+Ypqa2t1xx136I033ih0S3mJRCJasGCBYrGYYrGYDh06VOiWxrRjxw5FIhH5fD6dPXs2+/7AwIAaGhpUU1OjhQsXqqOjo4Bdjm60/uvr6xWNRrPHYf/+/QXscnRXrlzRunXrVFtbq1gspoaGBvX09EiaGsdgrP6L4hg4RWjFihXOwYMHHcdxnGeeecapq6srbEN5mj9/vvP6668Xug3XXn75ZScej9/Q99e//nXn4YcfdhzHcV599VWnurraGR4eLlCXoxut/+XLlzs//elPC9iZOx988IHz3HPPOZlMxnEcx3nsscecu+66y3GcqXEMxuq/GI5B0YVcf3+/EwwGswcyk8k4oVDIOX/+fGEby8NUC7lrPt73zJkznYGBgezrJUuWOL/61a8K0Jk7UzXkPu7UqVPOTTfd5DjO1DsGjnN9/8VwDIrudDUej2vevHkqLb264szn86m6uloXLlwocGf52bx5sxYtWqT77rtPly9fLnQ7eUskEspkMpozZ072vUgkMuWOw86dO7Vo0SJt2rRJ3d3dhW7HlZaWFq1du3bKHoNr/V9T6GNQdCEnKbtI/xpnit3Kd/z4cXV2dur06dOqrKzU1q1bC93SuEz149Da2qpz587pzJkzWrZsmdasWVPolnLas2ePurq6tHv3bklT7xh8vP+iOAYFnUeOoL+/3wkEAlP6dPWPXbx40Zk1a1ah23Dl46d7ZWVlU+pUKdfXBH6/33nnnXc+wY7y873vfc/54he/6Lz77rvZ96bSMRip/48rxDEoupnc3LlztXjxYj311FOSpPb2dkUiEUUikcI25tL777+v9957L/u6ra1NixcvLlxDE7Bx40YdOHBAknTq1Cn19fVp6dKlBe7KnXQ6rf7+/uzr9vZ2hUIhVVZWFrCr0e3bt09tbW164YUXVFFRkX1/qhyDkfovlmNQlMu63nzzTW3btk2JREKBQEBPPvmkbrvttkK35Up3d7fWr1+vjz76SI7jKBqN6tFHHy3qkN6+fbuOHj2qvr4+zZ49W7NmzdJbb72l/v5+bdmyRefPn9f06dP1xBNPaPny5YVu9wYj9d/Z2anly5crlUqppKREs2fP1r59+3T77bcXut0b9Pb2KhwOKxqNqry8XNLVtZ8nT56cEsdgtP5ffPHFojgGRRlyAOCVojtdBQAvEXIATCPkAJhGyAEwjZADYBohB8A0Qg6AaYQcANMIOQCmEXIATCPkAJj2v/dT4E6/T3AZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_batch = next(iter(valDataLoader)) # 1개 배치씩 가져옴\n",
    "\n",
    "#test_img[0] #그중에 첫번째\n",
    "test_img = test_batch[0][0] # 그중에 이미지\n",
    "plt.imshow(test_img.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "621c582f-eae3-4704-9e1f-4d51d6aa8be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6850, -8.3828, -1.3778,  2.2044, -3.7540, -1.9340, -7.0674,  7.7723,\n",
       "         -1.4259,  1.3060]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.forward(test_img)  ## tensor 로 나오기 때문에 앞([0]) 부분은 logit 값이고, 뒤는 tensor grad 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef1fae7-cd26-4fbb-9d12-a0059133f52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.7679e-05, 9.5819e-08, 1.0561e-04, 3.7971e-03, 9.8115e-06, 6.0554e-05,\n",
       "        3.5706e-07, 9.9430e-01, 1.0065e-04, 1.5462e-03],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(mm.forward(test_img)[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e590ef3-79a2-4d09-b63a-7622f902a5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = torch.softmax(mm.forward(test_img)[0], dim=0).detach() ## grad 등 불필요한 정보는 빼버리고 \n",
    "np.argmax(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81106c2-f04d-44c1-8ccf-85be36986d8d",
   "metadata": {},
   "source": [
    "실제로 class index 와 같이 나옴을 확인할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
