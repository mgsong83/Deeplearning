{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f6d213-ff65-4e6c-9497-85146076a3a1",
   "metadata": {},
   "source": [
    "## Lightning 모듈 내부 해부"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea13b79-bb01-4271-b916-158ff49bc81c",
   "metadata": {},
   "source": [
    "일반적인 pytorch 에서 학습 sequence 는 다음과 같다. (traning loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef1d1d1a-d81a-4443-b69b-d0142124d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop( dataloader, model, lossfn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "\n",
    "        pred = model(x) # model class 가 callable 한 경우! forward를 직접 불러도 결과는 같겠지만 hook 이 동장하지 않음!\n",
    "        loss = lossfn( pred, y) \n",
    "\n",
    "         \n",
    "        if batch%100 == 0 :\n",
    "            loss, current = loss.item(), batch*len(x)\n",
    "            print(f\"loss : { loss }, [{current}/{size}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8eab67-5979-456f-a297-ccbd70d416ec",
   "metadata": {},
   "source": [
    "반면 torch lightning 에서 학습을 정의하는 부분은 다음과 같다. (traning_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e847a-ffa9-4a48-b990-9be6208aeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the class \n",
    "    def traning_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x) ## callable 한 경우 forward 진행 \n",
    "        loss = loss_function(y_hat, y) ## loss_function 은 미리 주어져야함\n",
    "        self.log('traninig loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b374ae03-99f7-4b0b-ab31-f2ffac5c18bc",
   "metadata": {},
   "source": [
    "구조는 매우 간단하다. 먼저 데이터를 받고 (x, y를 batch로부터, 이 부분은 pl.traniner.fit 이 알아서 해준다.) 이를 모델에 넣어서 나온 값(예측 y값)과, 실제 y값 의 차이를 loss_function 함수에 넣어서 loss 를 계산하고 이를 return 한다. 별도로 back propagation 이나 optimizer 설정은 필요 없음에 유의하자! \n",
    "\n",
    "이와 완벽하게 똑같은 구조로 validation 또는 test 를 위한 내부 메쏘드 설정도 가능하다. 이름은 똑같이 validation_step, test_step 이라고 정의하면 되고 또한 traninig step 에서의 return 은 loss 이지만 만약 역전파가 필요없는 validation 이나 test 에서는 return 을 정의하지 않고, 결과만 log 에 표현해도 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2aaad4-347e-4222-ad70-be079385cfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3d99b05-6ca3-416a-ba9d-a19f11219ddd",
   "metadata": {},
   "source": [
    "### History Log\n",
    "Log 의 경우에는 다음의 문서에 잘 나와 있는데 (https://lightning.ai/docs/pytorch/stable/api_references.html#loggers), 각각 Step, Epoch level 에서 로깅이 가능하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb43e4f-3c8d-4c8f-9598-a75ea456c5f7",
   "metadata": {},
   "source": [
    "```python\n",
    "LightningModule.log(name, value,\n",
    "    prog_bar=False, logger=True, on_step=None, on_epoch=None, reduce_fx='mean',\n",
    "    enable_graph=False, sync_dist=False, sync_dist_group=None, add_dataloader_idx=True,\n",
    "    batch_size=None, metric_attribute=None, rank_zero_only=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b35ec-05fa-49fe-9ecd-0790bba81066",
   "metadata": {},
   "source": [
    "예를들어 다음의 3개의 차이를 보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaea737-499e-4ecb-b47b-0d584a987ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import ProgressBar\n",
    "\n",
    "## 1) \n",
    "def traninig_step( self, batch, batch_idx ) :\n",
    "    x, y = batch\n",
    "    y_hat = self(x)\n",
    "    loss = loss_ftn(y_hat, y)\n",
    "    self.log(\"loss\", loss, on_step = True, on_epoch = False ) ## Step level 에서 \n",
    "    return loss\n",
    "\n",
    "## 2)\n",
    "def training_step( self, batch, batch_idx ):\n",
    "    x, y = batch\n",
    "    y_hat = self(x)\n",
    "    loss = loss_ftn(y_hat, y)\n",
    "    self.log(\"loss\", loss, on_step=False, on_epoch = True ) ## Epoch level 마다 log\n",
    "    return loss\n",
    "\n",
    "## 3)\n",
    "def traning_step( self, batch, batch_idx ):\n",
    "    x, y = batch\n",
    "    y_hat = self(x)\n",
    "    loss = loss_ftn(y_hat, y)\n",
    "    acc = FM.accuracy(y_hat, y, task=\"multiclass\", num_classes = 10)\n",
    "    metrics = {'loss' : loss, 'acc' : acc }\n",
    "    self.log_dict( metrics, prog_bar = True ) # by default, on_step = True, on_epoch = False \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d386bf5-5ab5-4113-8a29-862e64a94f4c",
   "metadata": {},
   "source": [
    "log 를 활용한 1) , 2) 의 경우의 차이는 step 마다 logging을 할 건지, 또는 epoch 마다 logging 을 할 건지의 차이고, 기본적으로는 on_step 이 default 이다. 만약 log 에 보다 자세한 정보를 기록하고 싶다면 먼저 log 에 담길 정보를 dict 형식으로 만든 다음에 (3번처럼), 이를 log_dict 매소드를 사용해서 기록하면 된다. 또한 prog_bar 를 활성화하면 진행정도를 볼 수 있는데 (마치 Keras 처럼) log 를 사용하면 자주 활성화 시키는 옵션이다. 이를 이용해서 실제 MNIST 모델을 만들어서 모델이 어떻게 돌아가는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eec530-a15b-41ac-8763-32b475aff492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
