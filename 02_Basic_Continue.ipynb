{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca68c85-c1c0-4fa0-a51b-944af1be2675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msong\\miniconda3\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.accelerators import accelerator\n",
    "from torchmetrics import functional as FM\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [5, 3.5]\n",
    "plt.rcParams[\"font.size\"] = \"8\"\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def dataLoader(batch_size=128):\n",
    "    train_dataset = MNIST('', transform=transforms.ToTensor(), train=True, download=True) ## 한 번 인터넷으로 가져온걸 매번 가져올 필요가 없기 때문에 가져올때 download True 로 하면 다음 부터는 다운로드 된 데이터를 사용한다.\n",
    "    test_dataset = MNIST('', transform=transforms.ToTensor(), train=False, download=True)\n",
    "    trainDataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valDataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return (trainDataLoader,valDataLoader)\n",
    "\n",
    "trainDataLoader,valDataLoader = dataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f700da8-48f4-4173-b7ea-6f8150446571",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() ## 회귀가 아닌 분류의 문제이므로 cross entropy 를 loss 로 사용한다. \n",
    "\n",
    "class mymodel( pl.LightningModule ):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 28*28, 10 ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d35116-beeb-4271-a62c-15c8dbaa2a2f",
   "metadata": {},
   "source": [
    "# 모델 리뷰 \n",
    "\n",
    "바로 전번에 돌린 모델을 보면, 이상한 점이 두 개 있다. 전에 사용했던 keras의 경우 categorical data의 cross entropy 를 계산하기 위해서는 output 되는 값을 확률 분포로 바꿔주고 (logit -> softmax 를 통한 prob), 또한 target y 값도 one-hot encoding 을 통해서 같은 dimmension 으로 바꿔줘야 확률분포의 차이 즉 크로스 엔트로피가 계산이 되었다. 근데 여기서는 그냥 y_pred (선형결합으로 이루어진 ~ logit 값, dim=10)  과  y (target 값, dim=1) 를 그냥 넣어주었다. \n",
    "\n",
    "이게 가능한 이유는 파이토치에서 이 부분을 자동으로 해주기 때문이다. 다음을 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c70c16f-0af8-4274-8ffc-eecc658003c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1672241687774658"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 0.5, 0.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee6326-fe95-4f84-a38f-171baad4d1c1",
   "metadata": {},
   "source": [
    "이걸 softmax를 취하지 않은 형태의 tensor에 대해서 적용해도 별 문제 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2365c15-7199-4236-85d9-d4fdad10a945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1672241687774658"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "softmax_layer = nn.Softmax(dim=1)\n",
    "tensor_softmaxed = softmax_layer(tensor1)  ## [[0, 0.5, 0.5, 0]] 으로 변환됨 \n",
    "\n",
    "out = loss_function(tensor_softmaxed, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e9f32-fe36-4774-9880-f5d711a77569",
   "metadata": {},
   "source": [
    "물론 정확히 Softmax를 취한 값과는 값이 살짝 다른데 이유는 cross entropy 에서 실행되는 softmax 는 log_softmax 이기 때문이다. (하지만 어차피 loss 는 경향을 보는 거지 절대값 자체가 중요한건 아니기 때문에..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34a18e36-9e99-4de0-a9f3-1e4000fb9009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cd8b4-3a5e-45c9-87f0-746b7e19faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "softmax_layer = nn.LogSoftmax(dim=1)\n",
    "tensor_softmaxed = softmax_layer(tensor1)  ## [[0, 0.5, 0.5, 0]] 으로 변환됨 \n",
    "\n",
    "out = loss_function(tensor_softmaxed, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15d5f1-70c4-459d-a97e-cac3a8275daa",
   "metadata": {},
   "source": [
    "그런데 더 놀라운 사실은 이 뿐만 아니라 one-hot encoding도 알아서 해준다. (true_y 가 pred_y 와 같은 shape 이면 그대로 확률을 비교하고, 그렇지 않다면 one-hot encoding을 진행한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6345379-5c0e-4fad-9e6d-494789d886bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [1] )  ## 확률 벡터가 아니라, 그냥 index만 넣어줘도! \n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988aa1ec-ffee-45fd-a6d4-b1caf47e65f7",
   "metadata": {},
   "source": [
    "참 편리하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c138f0fa-bb3e-4836-970c-4a15f78c06a1",
   "metadata": {},
   "source": [
    "## 모댈 개선\n",
    "\n",
    "모델을 돌려서 학습까지 했지만, 뭐 나온게 없으므로 현 수준은 그냥 에러가 나지 않고 뭔가가 돌아갔다는 데 위안을 둬야한다. 이제 실제 성능을 확인하기 위해 몇가지를 개선하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7f0a2c-f623-4bc9-b6f3-318e0869b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() ## 회귀가 아닌 분류의 문제이므로 cross entropy 를 loss 로 사용한다. \n",
    "\n",
    "class mymodel2( pl.LightningModule ):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 28*28, 10 ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        acc = FM.accuracy(y_pred, y, task=\"multiclass\",num_classes=10) ## accuracy 를 계산해서\n",
    "        self.log_dict({'loss':loss, 'acc':acc})  # loss 와 함께 log 에 저장한다. \n",
    "        return loss \n",
    "         \n",
    "    def validation_step(self, batch, batch_idx):   ## training 과 같은 꼴이지만 역전파를 하지 않기 때문에 loss를 리턴하지 않는다.\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        acc = FM.accuracy(y_pred, y, task=\"multiclass\",num_classes=10)\n",
    "        self.log_dict({'val_loss':loss, 'val_acc':acc}) # 한 epoch 이 끝나면 validation 이 돌고, 이 때 역시 log에 결과를 저장한다.\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc192345-fe84-4f61-88f0-7dae8793a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = mymodel2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c2f53a-1047-49c9-b2b4-122d059e8f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 7.9 K \n",
      "--------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msong\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\msong\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e919f40ac864fb5b3e6faab87f2661b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 42.7 s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 3\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"firstModel\")  ## 결과를 확인하기 위해 log 를 저장했다. \n",
    "trainer = pl.Trainer(max_epochs=epochs, logger=logger, accelerator=\"auto\") # auto 로 설장하면 gpu 가 있으면 gpu를 사용해서 실행된다.\n",
    "trainer.fit(mm, trainDataLoader, valDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0f7964-3183-4f92-8dc9-9fea42e72914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.004386</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.765002</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.788308</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.580318</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493877</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.419665</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.392277</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>0</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.357626</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.323430</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0</td>\n",
       "      <td>449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "      <td>0.379396</td>\n",
       "      <td>0.9045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.324754</td>\n",
       "      <td>0.929688</td>\n",
       "      <td>1</td>\n",
       "      <td>499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.498448</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>1</td>\n",
       "      <td>549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.547592</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "      <td>599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.532515</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>1</td>\n",
       "      <td>649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1</td>\n",
       "      <td>699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.351515</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>1</td>\n",
       "      <td>749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.353887</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1</td>\n",
       "      <td>799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.306949</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>1</td>\n",
       "      <td>849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.405568</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>937</td>\n",
       "      <td>0.318939</td>\n",
       "      <td>0.9140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.355459</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>2</td>\n",
       "      <td>949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.363305</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.367591</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>2</td>\n",
       "      <td>1049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.289809</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>2</td>\n",
       "      <td>1099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.408345</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>2</td>\n",
       "      <td>1149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.289691</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>2</td>\n",
       "      <td>1199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.319145</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>2</td>\n",
       "      <td>1249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.234622</td>\n",
       "      <td>0.929688</td>\n",
       "      <td>2</td>\n",
       "      <td>1299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.305577</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>2</td>\n",
       "      <td>1349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.293409</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>2</td>\n",
       "      <td>1399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1406</td>\n",
       "      <td>0.295239</td>\n",
       "      <td>0.9190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       acc  epoch  step  val_loss  val_acc\n",
       "0   1.004386  0.835938      0    49       NaN      NaN\n",
       "1   0.765002  0.835938      0    99       NaN      NaN\n",
       "2   0.788308  0.804688      0   149       NaN      NaN\n",
       "3   0.580318  0.867188      0   199       NaN      NaN\n",
       "4   0.493877  0.898438      0   249       NaN      NaN\n",
       "5   0.419665  0.867188      0   299       NaN      NaN\n",
       "6   0.392277  0.914062      0   349       NaN      NaN\n",
       "7   0.357626  0.921875      0   399       NaN      NaN\n",
       "8   0.323430  0.937500      0   449       NaN      NaN\n",
       "9        NaN       NaN      0   468  0.379396   0.9045\n",
       "10  0.324754  0.929688      1   499       NaN      NaN\n",
       "11  0.498448  0.851562      1   549       NaN      NaN\n",
       "12  0.547592  0.875000      1   599       NaN      NaN\n",
       "13  0.532515  0.859375      1   649       NaN      NaN\n",
       "14  0.271900  0.937500      1   699       NaN      NaN\n",
       "15  0.351515  0.898438      1   749       NaN      NaN\n",
       "16  0.353887  0.937500      1   799       NaN      NaN\n",
       "17  0.306949  0.890625      1   849       NaN      NaN\n",
       "18  0.405568  0.898438      1   899       NaN      NaN\n",
       "19       NaN       NaN      1   937  0.318939   0.9140\n",
       "20  0.355459  0.921875      2   949       NaN      NaN\n",
       "21  0.363305  0.875000      2   999       NaN      NaN\n",
       "22  0.367591  0.906250      2  1049       NaN      NaN\n",
       "23  0.289809  0.921875      2  1099       NaN      NaN\n",
       "24  0.408345  0.898438      2  1149       NaN      NaN\n",
       "25  0.289691  0.914062      2  1199       NaN      NaN\n",
       "26  0.319145  0.914062      2  1249       NaN      NaN\n",
       "27  0.234622  0.929688      2  1299       NaN      NaN\n",
       "28  0.305577  0.914062      2  1349       NaN      NaN\n",
       "29  0.293409  0.921875      2  1399       NaN      NaN\n",
       "30       NaN       NaN      2  1406  0.295239   0.9190"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_num = logger.version  ## 버전에 따라서 폴더가 구분되므로, 최근에 돌린 버전을 확인 \n",
    "history = pd.read_csv(f'./logs/firstModel/version_{v_num}/metrics.csv') ## 파일 읽어오기 \n",
    "history ## 결과 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc0ac8-755d-4e50-b2d7-ece3e9153587",
   "metadata": {},
   "source": [
    "epoch에서 batch 단위로 로그가 저장됨을 알 수 있다. 기본적으로 training 이 한바퀴 돌고 validation 이 실행된다.\n",
    "epoch 단위로만 보고 싶다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147ba7d4-b1e3-4c01-ac8b-8273200785e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.323430</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.379396</td>\n",
       "      <td>0.9045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405568</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0.318939</td>\n",
       "      <td>0.9140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.293409</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.295239</td>\n",
       "      <td>0.9190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss       acc  val_loss  val_acc\n",
       "epoch                                       \n",
       "0      0.323430  0.937500  0.379396   0.9045\n",
       "1      0.405568  0.898438  0.318939   0.9140\n",
       "2      0.293409  0.921875  0.295239   0.9190"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.groupby('epoch').last().drop('step', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46513a-987e-492e-aea3-7c8d32c3ef45",
   "metadata": {},
   "source": [
    "MNIST 의 경우 너무 분류가 너무 쉬운 코드이기 때문에 별다른 trick 없이 10개의 dense layer 만으로도 90% 의 정분류율을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a9bf7-a9ec-4881-a0eb-1e2e7f6c556d",
   "metadata": {},
   "source": [
    "실제 어떻게 예측했는지를 좀 더 detail 하게 확인하기 위해서 validation 데이터를 하나 꺼내본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5ef87b7-e4ba-4c9d-b318-f2683c136803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x207222a6e10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_batch = next(iter(valDataLoader)) # 1개 배치씩 가져옴\n",
    "\n",
    "#test_img[0] #그중에 첫번째\n",
    "test_img = test_batch[0][0] # 그중에 이미지\n",
    "plt.imshow(test_img.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "621c582f-eae3-4704-9e1f-4d51d6aa8be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6502, -8.2960, -1.1830,  2.2145, -3.9479, -1.7938, -6.7980,  7.7487,\n",
       "         -1.4103,  1.3564]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.forward(test_img)  ## tensor 로 나오기 때문에 앞([0]) 부분은 logit 값이고, 뒤는 tensor grad 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ef1fae7-cd26-4fbb-9d12-a0059133f52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.2314e-05, 1.0697e-07, 1.3134e-04, 3.9257e-03, 8.2725e-06, 7.1304e-05,\n",
       "        4.7844e-07, 9.9401e-01, 1.0463e-04, 1.6644e-03],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(mm.forward(test_img)[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e590ef3-79a2-4d09-b63a-7622f902a5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = torch.softmax(mm.forward(test_img)[0], dim=0).detach() ## grad 등 불필요한 정보는 빼버리고 \n",
    "np.argmax(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81106c2-f04d-44c1-8ccf-85be36986d8d",
   "metadata": {},
   "source": [
    "실제로 class index 와 같이 나옴을 확인할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
