{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca68c85-c1c0-4fa0-a51b-944af1be2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.accelerators import accelerator\n",
    "from torchmetrics import functional as FM\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [5, 3.5]\n",
    "plt.rcParams[\"font.size\"] = \"8\"\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def dataLoader(batch_size=128):\n",
    "    train_dataset = MNIST('', transform=transforms.ToTensor(), train=True, download=True) ## 한 번 인터넷으로 가져온걸 매번 가져올 필요가 없기 때문에 가져올때 download True 로 하면 다음 부터는 다운로드 된 데이터를 사용한다.\n",
    "    test_dataset = MNIST('', transform=transforms.ToTensor(), train=False, download=True)\n",
    "    trainDataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valDataLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return (trainDataLoader,valDataLoader)\n",
    "\n",
    "trainDataLoader,valDataLoader = dataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f700da8-48f4-4173-b7ea-6f8150446571",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() ## 회귀가 아닌 분류의 문제이므로 cross entropy 를 loss 로 사용한다. \n",
    "\n",
    "class mymodel( pl.LightningModule ):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 28*28, 10 ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d35116-beeb-4271-a62c-15c8dbaa2a2f",
   "metadata": {},
   "source": [
    "# 모델 리뷰 \n",
    "\n",
    "바로 전번에 돌린 모델을 보면, 이상한 점이 두 개 있다. 전에 사용했던 keras의 경우 categorical data의 cross entropy 를 계산하기 위해서는 output 되는 값을 확률 분포로 바꿔주고 (logit -> softmax 를 통한 prob), 또한 target y 값도 one-hot encoding 을 통해서 같은 dimmension 으로 바꿔줘야 확률분포의 차이 즉 크로스 엔트로피가 계산이 되었다. 근데 여기서는 그냥 y_pred (선형결합으로 이루어진 ~ logit 값, dim=10)  과  y (target 값, dim=1) 를 그냥 넣어주었다. \n",
    "\n",
    "이게 가능한 이유는 파이토치에서 이 부분을 자동으로 해주기 때문이다. 다음을 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c70c16f-0af8-4274-8ffc-eecc658003c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1672241687774658"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 0.5, 0.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee6326-fe95-4f84-a38f-171baad4d1c1",
   "metadata": {},
   "source": [
    "이걸 softmax를 취하지 않은 형태의 tensor에 대해서 적용해도 별 문제 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2365c15-7199-4236-85d9-d4fdad10a945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1672241687774658"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "softmax_layer = nn.Softmax(dim=1)\n",
    "tensor_softmaxed = softmax_layer(tensor1)  ## [[0, 0.5, 0.5, 0]] 으로 변환됨 \n",
    "\n",
    "out = loss_function(tensor_softmaxed, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e9f32-fe36-4774-9880-f5d711a77569",
   "metadata": {},
   "source": [
    "물론 정확히 Softmax를 취한 값과는 값이 살짝 다른데 이유는 cross entropy 에서 실행되는 softmax 는 log_softmax 이기 때문이다. (하지만 어차피 loss 는 경향을 보는 거지 절대값 자체가 중요한건 아니기 때문에..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34a18e36-9e99-4de0-a9f3-1e4000fb9009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cd8b4-3a5e-45c9-87f0-746b7e19faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [[0, 1, 0., 0]] )\n",
    "\n",
    "softmax_layer = nn.LogSoftmax(dim=1)\n",
    "tensor_softmaxed = softmax_layer(tensor1)  ## [[0, 0.5, 0.5, 0]] 으로 변환됨 \n",
    "\n",
    "out = loss_function(tensor_softmaxed, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15d5f1-70c4-459d-a97e-cac3a8275daa",
   "metadata": {},
   "source": [
    "그런데 더 놀라운 사실은 이 뿐만 아니라 one-hot encoding도 알아서 해준다. (true_y 가 pred_y 와 같은 shape 이면 그대로 확률을 비교하고, 그렇지 않다면 one-hot encoding을 진행한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6345379-5c0e-4fad-9e6d-494789d886bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471824645996"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor( [[0, 111.5, 111.5, 0]] )\n",
    "tensor2 = torch.tensor( [1] )  ## 확률 벡터가 아니라, 그냥 index만 넣어줘도! \n",
    "\n",
    "out = loss_function(tensor1, tensor2)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988aa1ec-ffee-45fd-a6d4-b1caf47e65f7",
   "metadata": {},
   "source": [
    "참 편리하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c138f0fa-bb3e-4836-970c-4a15f78c06a1",
   "metadata": {},
   "source": [
    "## 모댈 개선\n",
    "\n",
    "모델을 돌려서 학습까지 했지만, 뭐 나온게 없으므로 현 수준은 그냥 에러가 나지 않고 뭔가가 돌아갔다는 데 위안을 둬야한다. 이제 실제 성능을 확인하기 위해 몇가지를 개선하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7f0a2c-f623-4bc9-b6f3-318e0869b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() ## 회귀가 아닌 분류의 문제이므로 cross entropy 를 loss 로 사용한다. \n",
    "\n",
    "class mymodel2( pl.LightningModule ):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 28*28, 10 ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        acc = FM.accuracy(y_pred, y, task=\"multiclass\",num_classes=10) ## accuracy 를 계산해서\n",
    "        self.log_dict({'loss':loss, 'acc':acc})  # loss 와 함께 log 에 저장한다. \n",
    "        return loss \n",
    "         \n",
    "    def validation_step(self, batch, batch_idx):   ## training 과 같은 꼴이지만 역전파를 하지 않기 때문에 loss를 리턴하지 않는다.\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        acc = FM.accuracy(y_pred, y, task=\"multiclass\",num_classes=10)\n",
    "        self.log_dict({'val_loss':loss, 'val_acc':acc}) # 한 epoch 이 끝나면 validation 이 돌고, 이 때 역시 log에 결과를 저장한다.\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc192345-fe84-4f61-88f0-7dae8793a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = mymodel2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8c2f53a-1047-49c9-b2b4-122d059e8f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 7.9 K \n",
      "--------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msong\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\msong\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468964b737fb45eea7296697d1b2ad18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 50s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 3\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"firstModel\")  ## 결과를 확인하기 위해 log 를 저장했다. \n",
    "trainer = pl.Trainer(max_epochs=epochs, logger=logger, accelerator=\"auto\") # auto 로 설장하면 gpu 가 있으면 gpu를 사용해서 실행된다.\n",
    "trainer.fit(mm, trainDataLoader, valDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e0f7964-3183-4f92-8dc9-9fea42e72914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.353746</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.370299</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.338049</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.433701</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.369326</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.433046</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0</td>\n",
       "      <td>349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.340230</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.145363</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0</td>\n",
       "      <td>449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "      <td>0.280730</td>\n",
       "      <td>0.9215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.280620</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>1</td>\n",
       "      <td>499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.172243</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>1</td>\n",
       "      <td>549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.284004</td>\n",
       "      <td>0.945312</td>\n",
       "      <td>1</td>\n",
       "      <td>599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.513360</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>1</td>\n",
       "      <td>649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.254950</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>1</td>\n",
       "      <td>699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.153484</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>1</td>\n",
       "      <td>749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.244694</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>1</td>\n",
       "      <td>799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.289717</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1</td>\n",
       "      <td>849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.288871</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>937</td>\n",
       "      <td>0.274612</td>\n",
       "      <td>0.9233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.177210</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>2</td>\n",
       "      <td>949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.260289</td>\n",
       "      <td>0.929688</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.421390</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>2</td>\n",
       "      <td>1049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.289128</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>2</td>\n",
       "      <td>1099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.257157</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>2</td>\n",
       "      <td>1149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.180783</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>2</td>\n",
       "      <td>1199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.305553</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>2</td>\n",
       "      <td>1249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.271280</td>\n",
       "      <td>0.929688</td>\n",
       "      <td>2</td>\n",
       "      <td>1299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.331421</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>2</td>\n",
       "      <td>1349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.313481</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>2</td>\n",
       "      <td>1399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1406</td>\n",
       "      <td>0.273362</td>\n",
       "      <td>0.9237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       acc  epoch  step  val_loss  val_acc\n",
       "0   0.236400  0.914062      0    49       NaN      NaN\n",
       "1   0.353746  0.898438      0    99       NaN      NaN\n",
       "2   0.370299  0.898438      0   149       NaN      NaN\n",
       "3   0.338049  0.906250      0   199       NaN      NaN\n",
       "4   0.433701  0.843750      0   249       NaN      NaN\n",
       "5   0.369326  0.914062      0   299       NaN      NaN\n",
       "6   0.433046  0.851562      0   349       NaN      NaN\n",
       "7   0.340230  0.937500      0   399       NaN      NaN\n",
       "8   0.145363  0.960938      0   449       NaN      NaN\n",
       "9        NaN       NaN      0   468  0.280730   0.9215\n",
       "10  0.280620  0.898438      1   499       NaN      NaN\n",
       "11  0.172243  0.968750      1   549       NaN      NaN\n",
       "12  0.284004  0.945312      1   599       NaN      NaN\n",
       "13  0.513360  0.898438      1   649       NaN      NaN\n",
       "14  0.254950  0.921875      1   699       NaN      NaN\n",
       "15  0.153484  0.968750      1   749       NaN      NaN\n",
       "16  0.244694  0.921875      1   799       NaN      NaN\n",
       "17  0.289717  0.906250      1   849       NaN      NaN\n",
       "18  0.288871  0.898438      1   899       NaN      NaN\n",
       "19       NaN       NaN      1   937  0.274612   0.9233\n",
       "20  0.177210  0.960938      2   949       NaN      NaN\n",
       "21  0.260289  0.929688      2   999       NaN      NaN\n",
       "22  0.421390  0.906250      2  1049       NaN      NaN\n",
       "23  0.289128  0.898438      2  1099       NaN      NaN\n",
       "24  0.257157  0.914062      2  1149       NaN      NaN\n",
       "25  0.180783  0.960938      2  1199       NaN      NaN\n",
       "26  0.305553  0.906250      2  1249       NaN      NaN\n",
       "27  0.271280  0.929688      2  1299       NaN      NaN\n",
       "28  0.331421  0.921875      2  1349       NaN      NaN\n",
       "29  0.313481  0.953125      2  1399       NaN      NaN\n",
       "30       NaN       NaN      2  1406  0.273362   0.9237"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_num = logger.version  ## 버전에 따라서 폴더가 구분되므로, 최근에 돌린 버전을 확인 \n",
    "history = pd.read_csv(f'./logs/firstModel/version_{v_num}/metrics.csv') ## 파일 읽어오기 \n",
    "history ## 결과 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc0ac8-755d-4e50-b2d7-ece3e9153587",
   "metadata": {},
   "source": [
    "epoch에서 batch 단위로 로그가 저장됨을 알 수 있다. 기본적으로 training 이 한바퀴 돌고 validation 이 실행된다.\n",
    "epoch 단위로만 보고 싶다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "147ba7d4-b1e3-4c01-ac8b-8273200785e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.145363</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0.280730</td>\n",
       "      <td>0.9215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.288871</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0.274612</td>\n",
       "      <td>0.9233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.313481</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.273362</td>\n",
       "      <td>0.9237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss       acc  val_loss  val_acc\n",
       "epoch                                       \n",
       "0      0.145363  0.960938  0.280730   0.9215\n",
       "1      0.288871  0.898438  0.274612   0.9233\n",
       "2      0.313481  0.953125  0.273362   0.9237"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.groupby('epoch').last().drop('step', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46513a-987e-492e-aea3-7c8d32c3ef45",
   "metadata": {},
   "source": [
    "MNIST 의 경우 너무 분류가 너무 쉬운 코드이기 때문에 별다른 trick 없이 10개의 dense layer 만으로도 90% 의 정분류율을 보여준다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
